{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def linear(a_in, W, b, g):\n",
    "    \"\"\"Fully connected layer with activation function.\n",
    "    \n",
    "    Args:\n",
    "        a_in: Input matrix (k, m) where k = # of observations, m = # of features.\n",
    "        W: Weight matrix (m, n) where n = # of neurons.\n",
    "        b: Bias vector (n,) to be reshaped for broadcasting.\n",
    "        g: Activation function.\n",
    "    \n",
    "    Returns:\n",
    "        a_out: Activated output matrix (k, n).\n",
    "    \"\"\"\n",
    "    return g(np.dot(a_in, W) + b.reshape(1, -1))  # Apply activation function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"Sigmoid activation function.\"\"\"\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Layer:\n",
      " [[0.92237644 0.77627751 0.41283563 0.11536843]\n",
      " [0.09340212 0.64104914 0.87879997 0.22657167]]\n",
      "Layer 1 output:\n",
      " [[0.85985814 0.7488289  0.83066615 0.79744036 0.72198051 0.91077923\n",
      "  0.79532635 0.8742238 ]\n",
      " [0.863263   0.68555883 0.74292146 0.76868005 0.73371433 0.84784821\n",
      "  0.72223137 0.83022051]]\n",
      "Layer 2 output:\n",
      " [[0.97671929 0.96558666 0.98265154 0.98487284 0.99333437 0.95276031\n",
      "  0.98283353 0.96500664 0.97810901]\n",
      " [0.97151284 0.96054218 0.97982007 0.98179527 0.99152412 0.94699108\n",
      "  0.97919205 0.95979165 0.97482949]]\n",
      "Layer 3 output:\n",
      " [[0.98884148 0.99320689 0.99166274 0.99762278 0.99464168 0.99479446\n",
      "  0.99645083]\n",
      " [0.98865615 0.99309403 0.99152809 0.9975705  0.99454456 0.99468561\n",
      "  0.99637121]]\n"
     ]
    }
   ],
   "source": [
    "# Test the linear layer with multiple layers in a loop\n",
    "num_layers = 3  # Define the number of layers\n",
    "layer_sizes = [np.random.randint(2, 10) for _ in range(num_layers + 1)]  # Random sizes for each layer\n",
    "\n",
    "k = np.random.randint(1, 5)  # Number of observations\n",
    "a = np.random.rand(k, layer_sizes[0])  # Input matrix (k, m)\n",
    "g = sigmoid  # Activation function\n",
    "\n",
    "print(\"Input Layer:\\n\", a)\n",
    "\n",
    "for i in range(num_layers):\n",
    "    W = np.random.rand(layer_sizes[i], layer_sizes[i + 1])  # Weight matrix\n",
    "    b = np.random.rand(layer_sizes[i + 1])  # Bias vector\n",
    "    a = linear(a, W, b, g)  # Compute output\n",
    "    print(f\"Layer {i+1} output:\\n\", a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "StockPrediction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
